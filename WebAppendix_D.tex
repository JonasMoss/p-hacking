\section*{Web Appendix D}

The publication bias and the \textit{p}-hacking models have been defined as a selection model and a mixture model, respectively. To appreciate the difference between the models, and relate the selection biases to them, consider the following example. We want to perform a meta-analysis based on 10 studies. In the ideal situation with no publication bias and no \textit{p}-hacking, 10 researchers perform their studies fairly, observe an effect size from its distribution $\phi(x_{i}\mid\theta_i,\sigma^2_{i})$ and publish the result. To perform our meta-analysis, we do not need much more than to estimate $\theta_0$ by using $\phi(x_{i}\mid\theta_i,\sigma^2_{i})$ and $\phi(\theta_{i}\mid\theta_0,\tau^2)$.

In a publication bias scenario, some of these 10 studies may not be published because their \textit{p}-values $u_i$ are too large. Let us say that the first 3 have a \textit{p}-value smaller than $0.025$ (group A), the following 4 between $0.025$ and $0.05$ (group B), and the remaining 3 larger than $0.05$ (group C). With an editor publishing all studies ($\rho_{1} = 1$) with $u_i < 0.025$, half ($\rho_{2} = 0.5$) of those with $0.025 \leq u_i < 0.05$ and one-third ($\rho_{3} = 0.33$) when $u_i \geq 0.05$, we would expect to see all 3 studies in group A, 2 of Group B and only one of group $C$. It is clear that the density is not anymore the original $\phi(x_{i}\mid\theta_i,\sigma^2_{i})$, but a transformed one $f(x_{i}\mid\theta_i,\sigma^2_{i})$ for which some values of $\theta_i$ are underrepresented by the effect of a selection mechanism. In this case, by the selection probability function $w(u_i) = 1_{[0,0.025)}(u_i) + 0.5 \cdot 1_{[0.25,0.05)}(u_i) + 0.33 \cdot 1_{[0.05, 1]}(u_i)$. In practice we do not know $\rho_{1}$, $\rho_{2}$ and $\rho_{3}$, and we need to estimate them in our model. In the ideal situation, they are all equal to 1, that means no selection bias, so $f(x_{i}\mid\theta_{i},\sigma^2_{i}) = \phi(x_{i}\mid\theta_i,\sigma^2_{i})$.

In the \textit{p}-hacking scenario, instead, we observe all our 10 studies, but not with their original effect size, because they have been modified in order to reach a specific significance value $\alpha$. In mathematical terms, $x_i$ does not come from the original Gaussian $\phi(x_{i}\mid\theta_i,\sigma^2_{i})$ but from a truncated Gaussian $\phi_\alpha^{\star}(x_{i}\mid\theta_i,\sigma^2_{i})$, where the truncation excludes the possibility to get an $u_i$ larger than $\alpha$. If every researchers decided to $p$-hack all studies at the same level, let us say $\alpha = 0.05$, we could make inference on $\theta_i$ using $\phi_{0.05}^{\star}(x_{i}\mid\theta,\sigma^2_{i})$ and $\phi(\theta_{i}\mid\theta_0,\tau^2)$. But researchers may choose to $p$-hack at a different $\alpha$, for example at $\alpha_1 = 0.025$ with probability $\pi_1=  0.1$, at $\alpha_2 = 0.05$ with probability $\pi_2 = 0.7$ and no-hack ($\alpha_3 = 1$) with probability $\pi_3 = 0.2$. So the density from which our 10 studies will be in this case a mixture of the three truncated Gaussian, with mixing distribution $\omega(\alpha) = 0.2 \cdot 1(\alpha = 0.025) + 0.7 \cdot 1(\alpha = 0.05) + 0.2 \cdot 1(\alpha = 1)$. Also in this case we cannot know the probabilities $\pi_1$, $\pi_2$ and $\pi_3$ in advance and we need to estimate them from the data. Here the ideal situation, no $p$-hacking, is $\pi_1 = \pi_2 = 0$ and $\pi_3 = 1$.

The main difference between the models is then related to $\theta_i$. The publication bias mechanism affects $\phi(x_{i}\mid\theta_i,\sigma^2_{i})$ once the study is done, i.e., we have already observed an instance of $\theta_i$. If the study is not selected, we generate a new $\theta_i$, i.e., we make a new study until we reach out 10. In contrast, in the $p$-hacking mechanism all studies are retained, just modified, so we keep all the generated $\theta_i$. This difference affects the distribution of $\theta_i$, $\phi(\theta_{i}\mid\theta_0,\tau^2)$. If $\theta_i = \theta$ in all studies (fixed effect meta-analysis), it does not matter.
