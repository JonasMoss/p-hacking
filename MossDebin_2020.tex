\documentclass[useAMS,usenatbib,referee]{biom}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage[colorlinks=true, citecolor = blue]{hyperref}      % hyperlinks
\usepackage{multirow, amssymb, amsmath, graphicx, arydshln, url}
\usepackage[T1]{fontenc}
\usepackage{natbib}


\author{Jonas Moss$^{*}$\email{jonasmgj@math.uio.no} and
Riccardo De Bin$^{**}$\email{debin@math.uio.no}\\
Department of Mathematics, University of Oslo, Moltke Moes vei 35, 0851 Oslo, Norway}

\def\bSig\mathbf{\Sigma}
\newcommand{\VS}{V\&S}
\newcommand{\tr}{\mbox{tr}}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lem}[theorem]{Lemma}
\newtheorem{thm}[theorem]{Theorem}
\newtheorem{exampl}[theorem]{Example}

\renewcommand{\sqrt}[1]{(#1)^{1/2}}

\title[Modelling publication bias and \textit{p}-hacking]{Modelling publication bias and \textit{p}-hacking}

\begin{document}

\date{{\it Received Month} 20XX. {\it Revised Month} 20XX.  {\it Accepted Month} 20XX.}

\pagerange{\pageref{firstpage}--\pageref{lastpage}} 
\volume{XX}
\pubyear{20XX}
\artmonth{Month}

\doi{10.1111/j.1541-0420.2005.00454.x}

\label{firstpage}

\begin{abstract}
Publication bias and \textit{p}-hacking are two well-known phenomena that strongly affect the scientific literature and cause severe problems in meta-analyses. Due to these phenomena, the assumptions of meta-analyses are seriously violated and the results of the studies cannot be trusted. While publication bias is almost perfectly captured by the weighting function selection model, \textit{p}-hacking is much harder to model and no definitive solution has been found yet. In this paper we propose to model both publication bias and \textit{p}-hacking with selection models. We derive some properties for these models, and we compare them formally and through simulations. Finally, two real data examples are used to show how the models work in practice.
\end{abstract}

\begin{keywords}
File drawer problem; Fishing for significance; Meta-analysis; Questionable research practices, Selection bias.
\end{keywords}

\maketitle

\section{Introduction}

Meta-analysis is the quantitative combination of information from different studies. Aggregating information from multiple studies brings higher statistical power, higher accuracy in estimation and greater reproducibility. Unfortunately, it is not always possible to believe in the results of meta-analyses, as some model assumptions may be seriously violated. In particular, a meta-analysis must not be based on a biased selection of studies. Publication bias \citep{sterling1959publication} and \textit{p}-hacking \citep{simmons2011false} are the most common phenomena that violate these assumptions. 

Publication bias, also known as the file drawer problem \citep[see, e.g.,][]{iyengar1988selection}, denotes that phenomenon when a study with a smaller \textit{p}-value is more likely to be published than a study with a higher \textit{p}-value. Publication bias is a well known issue, and several approaches have been proposed to tackle it. Two famous examples are the trim-and-fill \citep{duval2000trim} and fail-safe $N$ \citep{becker2005failsafe} methods, but neither of them explicitly model the publication selection mechanism. From a statistical point of view, the most important class of models which are used to deal with publication bias are selection models. They were first studied by \citet{hedges1984estimation} for $F$-distributed variables with a cut-off at $0.05$, and extended to the setting of $t$-values by \citet{iyengar1988selection}. \citet{hedges1992modeling} proposed a random effects publication bias model with more than one cut-off, while \citet{citkowicz2017parsimonious} used beta distributed weights. Other selection models include those of \cite{dear1992approach} and \cite{CopasShi2000}.

Publication bias is a well-known problem in several research areas, and therefore various approaches to solve the issue have been also proposed outside the statistical literature. Hailing from economics, the models PET and PET-PEESE \citep{stanley2014meta} are two models based on linear regression and an approximation of the selection mechanism based on the inverse Mill's ratio. From psychology, the \textit{p}-curve of \citet{simonsohn2014p} is a method that only looks at significant \textit{p}-values and judges whether their distribution shows sign of being produced by studies with insufficient power. The \textit{p}-curve for estimation \citep{simonsohn2014} is a fixed effect selection model with a significance cut-off at $0.05$ estimated by minimizing the Kolmogorov-Smirnov distance \citep{mcshane2016adjusting}. Another method from the psychology literature is \textit{p}-uniform \citep{van2015meta}, which is similar to the \textit{p}-curve. A recent study by \citet{carter2019correcting} compared several approaches and showed that the selection model works better than the others. However, not even the best method works well in every considered scenario. For more information on publication bias and a good review of large part of these methods we refer to the book by \citet{rothstein2006publication}.

In contrast, \textit{p}-hacking, sometimes also called \emph{questionable research practices} \citep{Sijtsma2016} and \emph{fishing for significance} \citep{Boulesteix2009}, occurs when the authors of a study manipulate results into statistical significance. \textit{p}-hacking can be done at the experimental stage, using for example optional stopping, or at the analysis stage, for instance by changing models or dropping out participants. Examples of \textit{p}-hacking can be found in \citet{simmons2011false}. While publication bias, at least that based on p-values, has been shown to be very often well captured by selection models such as that of \citet{hedges1992modeling} \citep[see the aforementiond study of][]{carter2019correcting}, \textit{p}-hacking is much harder to model. The aforementioned \textit{p}-curve approach by \citet{simonsohn2014p} has been used for \textit{p}-hacking as well, but it has been shown to be not reliable \citep{BrunsIoannidis2016}. Here we advocate the selection model approach and propose to use it to model both publication bias and \textit{p}-hacking. We derive some properties for these models and argue they are best handled by Bayesian methods. 

The paper is organized as follows: In Section \ref{sect:models} we define the framework and introduce the models, which are also theoretically compared. Further comparisons are presented through simulations in Section \ref{sect:simulations} and real data examples in Section \ref{sect:examples}. We end with some concluding remarks in Section \ref{sect:conclusions}.

\section{Models}\label{sect:models}

\subsection{Framework}
The main ingredient of a meta-analysis is a collection of exchangeable statistics $x_{i}$. Each statistic $x_{i}$ has density $f^{\star}(x_{i}\mid\theta_{i},\eta_{i})$, where $\eta_i$ is a known or unknown nuisance parameter and $\theta_{i}$ is an unknown parameter we wish to do inference on. This paper is about the fact that the true data-generating model $f^{\star}(x_{i}\mid\theta_{i},\eta_{i})$ is often not what it ideally should have been, such as a normal density. It has instead been transformed into something else by the forces of publication bias and \textit{p}-hacking. Our goal is to understand what it has been transformed into, and how we can estimate $\theta_{i}$ accordingly. The publication bias model of \citet{hedges1992modeling,iyengar1988selection} and the soon-to-be introduced \textit{p}-hacking model are models that transform the underlying densities, denoted by $f^{\star}(x_{i}\mid\theta_{i},\eta_{i})$, into new densities, $f_{i}(x_{i}\mid\theta_{i},\eta_{i})$. The results of this paper will be presented for normal densities, but they hold for any distribution. We only require the dependencies on a parameter of interest $\theta_{i}$ and that statistical inference on $\theta_{i}$ is the goal of the analysis.

The parameter $\theta_{i}$ is typically an effect size, such as a standardized mean difference. In a fixed effects meta-analysis, $\theta_{i}=\theta$ for all $i$. In a random effects meta-analysis, $\theta_{i}$ is drawn from an effect size distribution $p(\theta)$ common to all $i$, and the goal of the study is often to make inference on the parameters of the effect size distribution, for example on the mean $\theta_{0}$ and the standard deviation $\tau$ when $\theta_i \sim N(\theta_{0},\tau^2)$. If we marginalize away $\theta_{i}$ we will end up with a density on the form $f(x_{i}\mid \theta_{0},\sigma_{i}^{2}+\tau^{2})$, assuming $x_{i}$ is also from a normal distribution with standard deviation $\sigma_{i}$, i.e., $\eta_i = \sigma_i$, where we follow the usual practice of assuming $\sigma_i$ as known 
\citep{vanHouwelingen2002}. This is possible in our framework, but it turns out that an important property of the publication bias model gets lost, as marginalizing out the $\theta_{i}$s can mask the fact that the selection mechanism in the publication bias has an effect both on the effect size distribution and the individual densities $f_{i}(x_{i}\mid\theta_i, \sigma_i)$.

\subsection{The selection model}\label{sec:selectionModel}
Before introducing the publication bias and the $p$-hacking models, let us define the \emph{selection model}. Consider the statistic $x_i$ and its density $f^{\star}(x_{i})$, for the moment without the dependencies on $\theta_{i}$ and $\eta_{i}$. Let the \emph{selection variable} $s$ be a binary stochastic variable that equals $1$ if and only if $x_i$ is observed, e.g., in a publication bias scenario, if the paper containing $x_i$ has been accepted by an editor. When the selection only depends on $x_i$, the density of our observed statistic is $f_{i}(x_{i}) = p(s=1\mid x_i)/p(s=1)f^{\star}(x_{i})$. This is also known as a \emph{weighted distribution}, see e.g. \citet[][eq. 3.1]{rao1985weighted}, and can be interpreted as a rejection sampling model \citep{von1951various}.

% Making use of the notational convention $y_H = \{y_j, j\in H\}$, define the \emph{selection model} based on $H$ as
% \begin{equation}
% f_{H}(y)=\frac{p(s=1\mid y)}{p(s=1\mid y_{H^{c})}}p(y)\label{eq:H-selection model},
% \end{equation}
% where $H^c$ is the complement of $H$. It is easy to verify that $f_{H}(y)$ is a density for any $H$. The choice of $H$ strongly affects the form of $f_{H}(y)$, and, in general, $f_{H}(y) \neq f_{G}(y)$ for $H \neq G$.

% \begin{prop}
% \label{prop:Equal selection models}Two selection models based on the same $p(x)$ and $s$ are equal, i.e., $f_{H}(y)=f_{G}(y)$, if and only if $p(s=1\mid y_{H^{c}})=p(s=1\mid y_{G^{c}})$.
% In particular, $f_{H}(y)=p(y)$ if and only if $p(s=1\mid y_{H^{c}})=p(s=1\mid y)$.
% \end{prop}

% \begin{proof}
% Both results follow directly from Equation \eqref{eq:H-selection model}.
% \end{proof}


\subsection{The publication bias model} \label{subsect:publicationBias}

Imagine the publication bias scenario:
\begin{quote}
Alice is an editor who receives a study with a \textit{p}-value $u_i$. She knows her journals will suffer if she publishes many null-results, so she is disinclined to publish studies with large \textit{p}-values. Still, she will publish any result with some \textit{p}-value-dependent probability $w(u_i)$. Every study you will ever read in Alice's journal has survived this selection mechanism, the rest are lost forever.
\end{quote}
In this story, the underlying model $f^{\star}(x_{i}\mid\theta_{i},\eta_{i})$ is transformed into a publication bias model
\vspace{-9mm}
\begin{equation}\label{eq:Publication bias model}
f(x_{i}\mid\theta_{i},\eta_{i})\propto f^{\star}(x_{i}\mid\theta_{i},\eta_{i})w(u_i)
\end{equation}
by the selection probability $w(u_i) \in [0,1]$, which is a probability for each $u_i$. Here $u_i$ is a \textit{p}-value that depends on $x_{i}$ and maybe something else, such as the standard deviation of $x_{i}$, but does not depend on $\theta_{i}$. We can write the model using the selection variable $s$, as $w(u_i) = w(u_i(x_i, \eta_i \mid \theta_i)) = p(s=1 \mid x_i, \theta_i, \eta_i)$. Note that $w(u_i)$ cannot depend on $\theta_{i}$ since the editor has no way of knowing the parameter $\theta_{i}$; if she did, she would not have to look at the \textit{p}-values at all. The normalizing constant of model \eqref{eq:Publication bias model} is finite for any probability $w(u_i)$, hence $f$ is a \textit{bona fide} density.

An argument against the publication bias scenario is that publication bias does not act only through \textit{p}-values, but also through other features of the study such as language \citep{egger1998meta}
and originality \citep{callaham1998positive}. While this is true, the publication bias scenario seems to completely capture the idea of \textit{p}-value based publication bias. Even if other sources of publication bias exist, maybe acting through $x_{i}$ but not its \textit{p}-value, publication bias based on \textit{p}-values is a universally recognized problem, and a good place to start.

The kind of model sketched here is almost the same as the one of \citet{hedges1992modeling}, with the sole exception that \citet{hedges1992modeling} does not require $w(u_i)$ to be a probability, just that the integral of $f^{\star}(x_{i}\mid\theta_{i},\eta_{i})w(u_i)$ is finite, which can happen without $w(u_i)$ being a probability. We demand that $w(u_i)$ to be a probability since the intuitive publication bias scenario interpretation of the model disappears when $w(u_i)$ is not a probability. %Anyway, there are many choices for $w(u)$ even when we force it to be a probability. Assume $w^{\star}(u)$ is any bounded positive function in $\left[0,1\right]$, and define $w(u)=w^{\star}(u)/\textrm{sup}\{w^{\star}(u)\}$. Then $w(u)$ is a probability for each $u$, and fits right into the publication bias framework. An easy way to generate examples of such functions is to take density functions on $\left[0,1\right]$ and check if they are bounded. For instance, beta densities are bounded whenever both shape parameters are greater than $1$. The beta density is used in the publication bias model of \citet{citkowicz2017parsimonious}, but they do not demand it to be a probability.

Even if we know the underlying $f^{\star}(x_{i}\mid\theta_{i},\eta_{i})$ of model \eqref{eq:Publication bias model}, we will need to decide on what \textit{p}-value to use. Usually, the \textit{p}-value will be approximately a one-sided normal \textit{p}-value, but it might be something
else instead. A one-sided normal \textit{p}-value makes sense because most hypotheses have just one direction that is interesting. For instance, the effect of an antidepressant must be positive for the study to be publishable. A one-sided \textit{p}-value can also be used if the researchers reported a two-sided value, since $p=0.05$ for a two-sided hypothesis corresponds to $p=0.025$ for a one-sided hypothesis. We will use the one-sided normal \textit{p}-value in all examples in this paper.

Provided we know the underlying $f_{i}^{\star}$s and \textit{p}-values $u_i$, we only need to decide on the selection probability to have a fully specified model. \citet{hedges1992modeling} proposes the discrete selection probability
\begin{equation}
w(u_i\mid\rho,\alpha)=\sum_{j=1}^{J}\rho_{j}1_{(\alpha_{j-1},\alpha_{j}]}(u_i),
\end{equation}
where $\alpha$ is a vector with $0=\alpha_{0}<\alpha_{1}<\cdots<\alpha_{J}=1$ and $\rho$ is a non-negative vector with $\rho_{1}=1$. The interpretation of this selection probability is simple: When Alice reads the \textit{p}-value $u_i$, she finds the $j$ with $u_i\in(\alpha_{j-1},\alpha_{j}]$ and accepts the study with probability $\rho_{j}$. Related to this view, \citet{hedges1992modeling} proposed $\alpha_{[1,\dots,J-1]} = (0.001,0.005,0.01,0.05)$, as these \enquote{have particular salience for interpretation} \citep{hedges1992modeling}. In fact, a publication decision often depends on whether a \textit{p}-value crosses the $0.05$-threshold. His reason for using more split points than just $0.05$ is that \enquote{It is probably unreasonable to assume that much is known about the functional form of the weight function} \citep{hedges1992modeling}. While this is true, one may prefer, considering the bias-variance trade-off heuristic, to only use one split point at $0.05$, as done by \citet{iyengar1988selection} in their second weight function. Other reasons to prefer one split are ease of interpretation and presentation. Nevertheless, only using $0.05$ as a threshold for one-sided \textit{p}-values is problematic, as many published results are calculated using a two-sided \textit{p}-value instead. It seems therefore useful to add an additional splitting point at $0.025$, as a two-sided \textit{p}-value at that level corresponds to a one-sided \textit{p}-value of $0.05$. Ergo, in our examples we will use a two-step function selection probability $w(u_i\mid\rho)=1_{[0,0.025)}(u_i)+\rho_{2}1_{[0.025,0.05)}(u_i)+\rho_{3}1_{\left[0.05,1\right]}(u_i)$, where the selection probability when $u_i\in[0,0.025)$ is normalized to $1$ to make the model identifiable. Note, however, that the models are here presented in broad generality in order to allow for an arbitrary number $J$ of cut-offs \citep[this possibility is already implemented in the R package associated with this paper, \texttt{publipha}, see][for more details]{publipha}.

The following proposition shows the densities of the one-sided normal step function selection probability publication bias models, with fixed effects and with normal random effects, respectively. Here the notation $\phi_{[a,b)}(x\mid\theta,\sigma_i^2)$ indicates a normal truncated to $[a,b)$.
\begin{prop}
\label{prop:One-sided normal discrete probability vector publication bias model-1}
The density of an observation from a fixed effects one-sided normal step function selection probability publication bias model is
\begin{equation}\label{eq:Fixed effects, publication bias}
f(x_{i}\mid\theta,\sigma_{i}) = \sum_{j=1}^{J}\pi_{j}^\star\phi_{[\Phi^{-1}(1-\alpha_{j}),\Phi^{-1}(1-\alpha_{j-1}))}(x_{i}\mid\theta,\sigma_{i}^2),
\end{equation}
where $\pi_{j}^{\star}=\rho_{j}\frac{\Phi(c_{j-1}\mid\theta,\sigma_{i}^2)-\Phi(c_{j}\mid\theta,\sigma_{i}^2)}{\sum_{j=1}^{N}\rho_{j}\left[\Phi(c_{j-1}\mid\theta,\sigma_{i}^2)-\Phi(c_{j}\mid\theta,\sigma_{i}^2)\right]}$ and $c_{j}=\Phi^{-1}(1-\alpha_{j})$.

The density of an observation from the one-sided normal step function selection probability publication bias model with normal random effects and parameters $\sigma_{i},\theta_{0},\tau,$ is
\begin{equation}\label{eq:Random effects, publication bias}
f(x_i\mid\theta_{0},\tau,\sigma_{i})=\sum_{j=1}^{J}\pi_{j}^{\star}(\theta_0,\tau,\sigma_{i})\phi_{[\Phi^{-1}(1-\alpha_{j}),\Phi^{-1}(1-\alpha_{j-1}))}(x\mid\theta_{0},\tau^{2}+\sigma_{i}^{2}),
\end{equation}
where $\pi_{j}^{\star}(\theta_0,\tau,\sigma_{i})=\rho_{j}\frac{\Phi(c_{j-1}\mid\theta_{0},\tau^{2}+\sigma_{i}^{2})-\Phi(c_{j}\mid\theta_{0},\tau^{2}+\sigma_{i}^{2})}{\sum_{j=1}^{J}\rho_{j}\left[\Phi(c_{j-1}\mid\theta_{0},\tau^{2}+\sigma_{i}^{2})-\Phi(c_{j}\mid\theta_{0},\tau^{2}+\sigma_{i}^{2})\right]}$.
\end{prop}

Here $f(x_i\mid\theta_{0},\tau,\sigma_i)$ is not equal to $\int f(x_{i}\mid\theta_{i},\sigma_{i})\phi(\theta_{i}\mid\theta_{0},\tau)d\theta_{i}$, as it might have been expected. See the Web Appendix B for more details.

\subsection{The \textit{p}-hacking model}\label{subsect:p-hacking}
Imagine the \textit{p}-hacking scenario:
\begin{quote}
Bob is an astute researcher who is able to \textit{p}-hack any study to whatever level of significance he wishes. Whenever Bob does his research, he decides on a significance level to reach by drawing an $\alpha$ from a distribution $\omega$. Then he \textit{p}-hacks his study to this $\alpha$-level.
\end{quote}
In this scenario the original density $f^{\star}(x_{i}\mid\theta_{i},\eta_{i}, u_i)$
is transformed into the \textit{p}-hacked density
\begin{equation}\label{eq:p-hacking model}
f(x_{i}\mid\theta_{i},\eta_{i})=\int_{[0,1]}f_\alpha^{\star}(x_{i}\mid\theta_{i},\eta_{i}, u_i)d\omega(\alpha),
\end{equation}
where $f_\alpha^{\star}$ is the density $f^{\star}$ truncated so that the \textit{p}-value $u_i\in\left[0,\alpha\right]$, with $\alpha \in [0,1]$. As shown by the integral, we marginalize out $\alpha$ by considering its distribution $\omega$. As described in the \textit{p}-hacking scenario, indeed, $\alpha$ is drawn from $\omega$, which can therefore be seen as the \emph{propensity to p-hack} and might depend on covariates, $\omega(\alpha \mid u_i, \eta_i)$. In contrast, it should not depend on $\theta_{i}$, as the researcher cannot know the true effect size of his study. While publication bias model (\ref{eq:Publication bias model}) is a selection model, the \textit{p}-hacking model (\ref{eq:p-hacking model}) is clearly a mixture model. The publication bias can also be written as a mixture model on the same form as the \textit{p}-hacking model, but then $\omega$ will depend on $\theta$, see the Web Appendix B. We stress the fact that the model \eqref{eq:p-hacking model} is not a publication bias model. Although the \textit{p}-hacking model can be written as a selection model (that is, on the form of \eqref{eq:Publication bias model}), in general the publication probability will depend on the true effect size, which violates an obvious condition for a model to be considered a publication bias model.

Just as the publication bias model requires a choice of $w$, the \textit{p}-hacking model requires a choice of $\omega$. A \textit{p}-hacking scientist is motivated to \textit{p}-hack to the $0.05$ level, maybe to the $0.01$ or $0.025$, but never to a level such as $0.07$ or $0.37$. This motivates the discrete \textit{p}-hacking probability distribution
$$\omega(\alpha\mid\pi)=\sum_{j=1}^{J}\pi_{j}1_{(0,\alpha_{j}]}(\alpha)$$
for some $j$-ary vector $\alpha$ satisfying $0<\alpha_{1}<\alpha_{2}<\cdots<\alpha_{J}=1$,
and $j$-ary vector of probabilities $\pi$. The resulting density is 
\[
f(x_{i}\mid\theta_{i},\eta_{i})=\sum_{j=1}^{J}\pi_{j}\left(\int_{u_i\in(0,\alpha_{j}]}f^\star(x_{i}\mid\theta_{i},\eta_{i}, u_i)d\omega(\alpha)\right)^{-1}f^\star(x_{i}\mid\theta_{i},\eta_{i}, u_i)1_{(0,\alpha_{j}]}(u_i).
\]
Using a reasoning entirely analogous to that of Section \ref{subsect:publicationBias}, we suggest to use an $\omega$ only based on the two splitting points $0.025$ and $0.05$, $\omega(u_i\mid\pi) = \pi_11_{[0,0.025]}(u_i) + \pi_{2}1_{(0,0.05]}(u_i) + \pi_{3}1_{(0,1]}(u_i)$, but the model below is presented in broad generality ($J$ cut-offs).

The density of an observation from a fixed effects one-sided normal discrete probability \textit{p}-hacking model is
\begin{eqnarray}
f(x_{i}\mid\theta,\sigma_{i}) & = & \sum_{j=1}^{J}\pi_{j}\phi_{[\Phi^{-1}(1-\alpha_{j}),\Phi^{-1}(1-\alpha_{j-1}))}(x_{i}\mid\theta,\sigma_{i}),\label{eq:Fixed effects, p-hacking}
\end{eqnarray}
while there is no closed form for the density of its random effect version.

\subsection{The difference between the models\label{subsec:Selection sets, meta analysis}}

% Things get more complicated when the problem also involve other quantities, such as the study-specific parameter $\theta_i$ or nuisance parameter $\eta_i$, as we need to specify exactly which variables are resampled when $s=0$.

% In our models, $\eta_i$ will never be resampled, leaving us with three options. First, $\theta_i$ and $x_i$ could be resampled when $s = 0$; this is what happens in a publication bias model. Second, only $x_i$ could be resampled when $s=0$, as happens in the \textit{p}-hacking model. Third, if no variable is resampled at all, the underlying model is not modified at all.

In the random effects publication bias model a completely new study is done whenever the last one failed to be published. In the event that $s=0$ and the study fails to be published, a new effect size $\theta_i$ is sampled from the effect size distribution, and then a new $x_i$ from $N(\theta_i,\sigma_i)$. As a consequence, the modified effect size distributed $p^\star(\theta_i\mid \sigma_i)$ will generally not equal the original effect size distribution, as
\[
p^\star(\theta_i\mid \sigma_i)=\int\frac{p(s=1\mid x, \sigma_i)}{p(s=1\mid\sigma_i)}f(x_i \mid \theta_i, \sigma _i) p(\theta_i) dx\neq p(\theta_i).
\]

The dependence on $\sigma_i$ in $p^\star(\theta_i\mid \sigma_i)$ cannot be removed. The modified effect size distribution will be skewed towards favourable $\theta_i$s. Publication bias does only selects studies that worked due lucky rolls, but selects studies that worked due to good underlying circumstances, as captured by the true effect size $\theta_i$. Even if we somehow new all the $\theta_i$s corresponding to our sample of $x_i$s, the mean of these $\theta_i$s would be larger than the mean of the underlying effect size distribution. In effect, the observed effect size distribution cannot be used directly to predict the value of a new draw from the true effect size distribution.

% We care about which variables are resampled when $s=0$ since the effect size distribution $p(\theta)$ can be modified. In particular, when $\theta_i$ and $x_i$ are resampled when $s=0$, the effect size distributed $p^\star(\theta_i)$ is modified along with likelihood $f(x_i\mid \theta_i,\eta_i$:
% \[
% p^\star(\theta_i\mid \sigma_i)=\int\frac{p(s=1\mid x, \sigma_i)}{p(s=1\mid\sigma_i)}f(x_i \mid \theta_i, \sigma _i) p(\theta_i) dx\neq p(\theta_i).
% \]

% When the original effect size distribution is normal $N(\theta_0,\tau)$, the modified effect size distribution will depend on $\sigma_i$ and might be deviate considerably from $N(\theta_0,\tau)$. To in

The \textit{p}-hacking model does not modify the effect size distribution. The \textit{p}-hacker will hack his study all the way to significance, regardless of $\theta_i$. In this case, $\theta_i$ is not resampled when $s=0$, and
\[
p^\star(\theta_i\mid \sigma_i)=\int\frac{p(s=1\mid x, \sigma_i)}{p(s=1\mid \theta_i, \sigma_i)}f(x_i \mid \theta_i, \sigma _i) p(\theta_i)dx = p(\theta_i).
\]

The publication bias model defined in Proposition \ref{prop:One-sided normal discrete probability vector publication bias model-1} and the \textit{p}-hacking model are equivalent when $\sigma_{i}$ is fixed across studies. This holds both for the fixed and random effects models. To see this, let $\pi$ be any probability vector for the \textit{p}-hacking model and solve the invertible linear system $\pi^{\star}(\rho)=\pi$ for $\rho$. There is no guarantee for the models to be equivalent when $\sigma_{i}$ is not fixed. Finally, Web Appendix C addresses the issue of identifiability, and shows that the two models are identifiable under weak conditions on $f$.


\section{Simulations}\label{sect:simulations}

We want to answer these three questions about the \textit{p}-hacking and publication bias models: (1) Do they work even in the absence of \textit{p}-hacking and publication bias? Although we know these phenomena are ubiquitous and should always be corrected for, it is still important that the models do not distort the results when there is no publication bias or \textit{p}-hacking. (2) How do they behave in extreme situations, in particular when $n$ is small and the heterogeneity is large? (3) Are the models distinguishable in practice? Does the \textit{p}-hacking model work under the publication bias scenario and vice versa?

\subsection{Settings}
We generate data under three scenarios: (i) With no publication bias nor \textit{p}-hacking, using the normal random effect meta-analysis model. (ii) Under the presence of publication bias, using model \eqref{eq:Random effects, publication bias}. (iii) Under presence of \textit{p}-hacking, using the random effects normal \textit{p}-hacking model. The study-specific variances $\sigma_{i}^{2}$ are sampled uniformly from $\left\{ 20,\ldots80\right\} $. The size of the meta-analyses are $n = 5, 30, 100$, corresponding to small, medium and large meta-analyses, while the means for the effect size distribution are $0, 0.2, 0.8$. The value $\theta_0 = 0$ corresponds to no expected effect, while the positive $\theta_0$s are the cut-off for small and large effect sizes of \citet[][pages 24 -- 27]{cohen1988statistical}. The standard deviations of the random effects distributions are $\tau=0.1$ and $\tau=0.5$. While $\tau = 0.1$ is a reasonable amount of heterogeneity, $\tau=0.5$ is a large amount of heterogeneity that provides a challenge for the models. The probability of acceptance of a paper are simulated to be $1$ if the \textit{p}-value is between $0$ and $0.025$, $0.7$ if the \textit{p}-value is between $0.025$ and $0.05$, and $0.1$ otherwise. For the same intervals, the \textit{p}-hacking probabilities are $0.6$, $0.3$ and $0.1$. 

In addition to the classical uncorrected model for meta-analysis, for each parameter combination we estimate the \textit{p}-hacking model and the publication bias model using Bayesian methods. While a frequentist approach is in theory possible, it may lead to poor results if ad-hoc penalizations or bias corrections are not implemented. See  \citet[Appendix, 1]{mcshane2016adjusting} and \citet{Moss2019} for further details. All models have normal likelihoods and normal effect size distributions. We use one-sided significance cut-offs at $0.025$ and $0.05$ for both the publication bias and the \textit{p}-hacking models. We use standard normal priors for $\theta_0$, a standard half normal prior for $\tau$, and, in the \textit{p}-hacking model, a uniform Dirichlet prior for $\pi$. For the $\rho$ in the publication bias model we use a a uniform Dirichlet that constrains $\rho_{1}\geq\ldots\geq\rho_{j}$. That is, the publication probability is a decreasing function of the \textit{p}-value.

All of these priors are reasonable. A standard normal for $\theta_0$ is reasonable because we know that $\theta_0$ has a small magnitude in pretty much any meta-analysis, and most are clustered around $0$. A half normal prior for $\tau$ is also reasonable, as $\tau$ is much more likely to be very small than very big. The priors for $\rho$ and $\pi$ are harder to reason about, but a uniform Dirichlet seems like a natural and neutral choice. These are the standard prior of the $\mathtt{R}$ package $\mathtt{publipha}$ \citep{publipha}, which we used for all computations. $\mathtt{publipha}$ uses $\mathtt{STAN}$ \citep{Carpenter2017-cf} to estimate the models, and each estimation uses $8$ chains.

The number of simulations is $N = 100$ for each parameter combination. The code used to run the simulations is available in the Online Supporting Information and in an OSF repository (\url{https://osf.io/tx8qn/}).

\subsection{Results}
\paragraph{No publication bias, no \textit{p}-hacking.} The results under this scenario are reported in Table \ref{tab:Simulation_classical}. When the amount of heterogeneity is reasonable ($\tau = 0.1$) both the \textit{p}-hacking and the publication bias perform well. The publication bias model performs slightly worse than the \textit{p}-hacking model when the mean effect size is large ($\theta_0 = 0.8$) and the number of studies small ($n=5$), but it catches up as $n$ increases. With $\tau = 0.5$, the \textit{p}-hacking model outperforms the publication bias model, with the latter tending to underestimate the mean effect. While increasing $n$ alleviates the problem, there is still a substantial underestimation of $\theta_0$ even in the case of $n = 100$. In contrast, both models seem to estimate $\tau$ pretty well. Obviously, without any publication bias or $p$-hacking, the classical uncorrected model gives good results.

\begin{table}[ht]
\centering
\caption{{\bf No publication bias, no 
                    \textit{p}-hacking.} Posterior means 
                    and standard deviations from the \textit{p}-hacking, 
                    publication bias, and uncorrected models when the data are simulated 
                    from the normal random effects meta-analysis model.} 
\label{tab:Simulation_classical}
\begin{tabular}{lllrrrrrr}
   \multicolumn{3}{r}{\textbf{True values}} & 
       \multicolumn{2}{c}{\textbf{\textit{p}-hacking model}} &
       \multicolumn{2}{c}{\textbf{Publication bias model}} &
       \multicolumn{2}{c}{\textbf{Uncorrected model}}\\$\tau$ & $\theta_0$ & $n$ & \multicolumn{1}{c}{$\widehat{\theta_0}$} & \multicolumn{1}{c}{$\widehat{\tau}$} & \multicolumn{1}{c}{$\widehat{\theta_0}$} & \multicolumn{1}{c}{$\widehat{\tau}$} & \multicolumn{1}{c}{$\widehat{\theta_0}$} & \multicolumn{1}{c}{$\widehat{\tau}$} \\ 
   \hline
\multirow{9}{*}{$0.1$} & \multirow{3}{*}{$0$} & 5 & -0.03 (0.09) & 0.18 (0.07) & -0.06 (0.08) & 0.13 (0.06) & 0.00 (0.09) & 0.19 (0.08) \\ 
   &  & 30 & -0.01 (0.03) & 0.08 (0.03) & -0.02 (0.03) & 0.07 (0.03) & 0.00 (0.04) & 0.10 (0.04) \\ 
   &  & 100 & -0.01 (0.02) & 0.08 (0.03) & -0.01 (0.02) & 0.07 (0.02) & 0.00 (0.02) & 0.10 (0.03) \\ 
   \cdashline{3-9}
 & \multirow{3}{*}{$0.2$} & 5 & 0.12 (0.08) & 0.21 (0.08) & 0.09 (0.07) & 0.17 (0.08) & 0.20 (0.08) & 0.19 (0.08) \\ 
   &  & 30 & 0.17 (0.04) & 0.09 (0.04) & 0.15 (0.03) & 0.09 (0.04) & 0.20 (0.03) & 0.10 (0.04) \\ 
   &  & 100 & 0.18 (0.02) & 0.09 (0.03) & 0.17 (0.02) & 0.09 (0.03) & 0.20 (0.02) & 0.10 (0.02) \\ 
   \cdashline{3-9}
 & \multirow{3}{*}{$0.8$} & 5 & 0.78 (0.08) & 0.21 (0.10) & 0.63 (0.15) & 0.34 (0.14) & 0.79 (0.07) & 0.20 (0.08) \\ 
   &  & 30 & 0.80 (0.04) & 0.11 (0.04) & 0.80 (0.04) & 0.11 (0.04) & 0.80 (0.04) & 0.10 (0.04) \\ 
   &  & 100 & 0.80 (0.02) & 0.10 (0.03) & 0.80 (0.02) & 0.10 (0.03) & 0.80 (0.02) & 0.09 (0.03) \\ 
   \cline{2-9}
\multirow{9}{*}{$0.5$} & \multirow{3}{*}{$0$} & 5 & -0.03 (0.20) & 0.59 (0.21) & -0.21 (0.17) & 0.53 (0.21) & 0.01 (0.20) & 0.61 (0.20) \\ 
   &  & 30 & -0.03 (0.09) & 0.51 (0.08) & -0.14 (0.09) & 0.47 (0.08) & -0.01 (0.09) & 0.51 (0.07) \\ 
   &  & 100 & -0.02 (0.05) & 0.50 (0.04) & -0.08 (0.06) & 0.48 (0.04) & 0.00 (0.05) & 0.50 (0.04) \\ 
   \cdashline{3-9}
 & \multirow{3}{*}{$0.2$} & 5 & 0.10 (0.22) & 0.57 (0.20) & -0.09 (0.19) & 0.54 (0.19) & 0.16 (0.21) & 0.58 (0.19) \\ 
   &  & 30 & 0.15 (0.10) & 0.53 (0.08) & 0.02 (0.10) & 0.51 (0.08) & 0.19 (0.10) & 0.52 (0.08) \\ 
   &  & 100 & 0.19 (0.05) & 0.51 (0.04) & 0.11 (0.06) & 0.49 (0.04) & 0.21 (0.05) & 0.50 (0.04) \\ 
   \cdashline{3-9}
 & \multirow{3}{*}{$0.8$} & 5 & 0.68 (0.23) & 0.62 (0.21) & 0.35 (0.23) & 0.74 (0.21) & 0.72 (0.21) & 0.59 (0.21) \\ 
   &  & 30 & 0.78 (0.10) & 0.52 (0.08) & 0.60 (0.14) & 0.60 (0.08) & 0.80 (0.09) & 0.50 (0.07) \\ 
   &  & 100 & 0.79 (0.05) & 0.51 (0.04) & 0.70 (0.07) & 0.55 (0.04) & 0.80 (0.05) & 0.50 (0.04) \\ 
   \hline
\end{tabular}
\end{table}

\paragraph{Publication bias} Overall, the publication bias model outperforms the \textit{p}-hacking model when the data are generated from the publication bias model, but not by much, see Table \ref{tab:Simulation_pb}. When $\tau = 0.5$ the \textit{p}-hacking model tends to overestimates $\theta_0$ while the publication bias model tends to underestimate it. The overestimation of the \textit{p}-hacking model is most extreme when $\theta_0 = 0.2$, but not as strong as the classical uncorrected model. When $\tau = 0.1$, the publication bias and $p$-hacking models produce almost indistinguishable results, ouperforming the uncorrected model (especially if the effect $\theta_0$ is null or small). Just as in the \textit{p}-hacking scenario, both models estimate $\tau$ reasonably well.

\begin{table}[ht]
\centering
\caption{{\bf Publication bias.} 
                    Posterior means and standard deviations from the 
                    \textit{p}-hacking, publication bias, and uncorrected models 
                    when the data are simulated from the publication 
                    bias model with cut-offs at $0.025$ and $0.05$, 
                    with selection probabilities equal to $1$, $0.7$, 
                    and $0.1$ in the intervals $[0, 0.025)$, $[0.025, 0.05)$, 
                    and $[0.5, 1]$.} 
\label{tab:Simulation_pb}
\begin{tabular}{lllrrrrrr}
   \multicolumn{3}{r}{\textbf{True values}} & 
       \multicolumn{2}{c}{\textbf{\textit{p}-hacking model}} &
       \multicolumn{2}{c}{\textbf{Publication bias model}} &
       \multicolumn{2}{c}{\textbf{Uncorrected model}}\\$\tau$ & $\theta_0$ & $n$ & \multicolumn{1}{c}{$\widehat{\theta_0}$} & \multicolumn{1}{c}{$\widehat{\tau}$} & \multicolumn{1}{c}{$\widehat{\theta_0}$} & \multicolumn{1}{c}{$\widehat{\tau}$} & \multicolumn{1}{c}{$\widehat{\theta_0}$} & \multicolumn{1}{c}{$\widehat{\tau}$} \\ 
   \hline
\multirow{9}{*}{$0.1$} & \multirow{3}{*}{$0$} & 5 & -0.01 (0.10) & 0.23 (0.08) & -0.01 (0.07) & 0.18 (0.07) & 0.13 (0.08) & 0.23 (0.10) \\ 
   &  & 30 & 0.02 (0.04) & 0.12 (0.05) & 0.01 (0.04) & 0.10 (0.04) & 0.13 (0.04) & 0.16 (0.04) \\ 
   &  & 100 & 0.02 (0.03) & 0.12 (0.03) & 0.00 (0.02) & 0.10 (0.03) & 0.13 (0.02) & 0.16 (0.02) \\ 
   \cdashline{3-9}
 & \multirow{3}{*}{$0.2$} & 5 & 0.10 (0.15) & 0.30 (0.09) & 0.10 (0.07) & 0.21 (0.08) & 0.32 (0.07) & 0.16 (0.08) \\ 
   &  & 30 & 0.22 (0.05) & 0.11 (0.05) & 0.19 (0.05) & 0.09 (0.04) & 0.33 (0.03) & 0.06 (0.03) \\ 
   &  & 100 & 0.23 (0.03) & 0.10 (0.04) & 0.20 (0.04) & 0.09 (0.03) & 0.33 (0.01) & 0.04 (0.02) \\ 
   \cdashline{3-9}
 & \multirow{3}{*}{$0.8$} & 5 & 0.77 (0.08) & 0.20 (0.08) & 0.62 (0.14) & 0.32 (0.12) & 0.78 (0.07) & 0.19 (0.07) \\ 
   &  & 30 & 0.80 (0.03) & 0.10 (0.04) & 0.79 (0.03) & 0.10 (0.04) & 0.80 (0.03) & 0.10 (0.03) \\ 
   &  & 100 & 0.80 (0.02) & 0.10 (0.02) & 0.80 (0.02) & 0.10 (0.02) & 0.80 (0.02) & 0.10 (0.02) \\ 
   \cline{2-9}
\multirow{9}{*}{$0.5$} & \multirow{3}{*}{$0$} & 5 & 0.34 (0.21) & 0.53 (0.20) & 0.04 (0.22) & 0.56 (0.18) & 0.42 (0.17) & 0.47 (0.23) \\ 
   &  & 30 & 0.36 (0.10) & 0.48 (0.09) & 0.01 (0.19) & 0.50 (0.08) & 0.43 (0.08) & 0.42 (0.09) \\ 
   &  & 100 & 0.36 (0.04) & 0.47 (0.04) & -0.01 (0.10) & 0.50 (0.04) & 0.43 (0.04) & 0.42 (0.04) \\ 
   \cdashline{3-9}
 & \multirow{3}{*}{$0.2$} & 5 & 0.42 (0.21) & 0.54 (0.22) & 0.12 (0.22) & 0.59 (0.19) & 0.50 (0.18) & 0.46 (0.21) \\ 
   &  & 30 & 0.50 (0.07) & 0.44 (0.08) & 0.16 (0.18) & 0.51 (0.09) & 0.56 (0.06) & 0.38 (0.08) \\ 
   &  & 100 & 0.51 (0.04) & 0.42 (0.04) & 0.19 (0.10) & 0.50 (0.05) & 0.57 (0.04) & 0.37 (0.04) \\ 
   \cdashline{3-9}
 & \multirow{3}{*}{$0.8$} & 5 & 0.81 (0.22) & 0.56 (0.19) & 0.47 (0.27) & 0.71 (0.20) & 0.86 (0.19) & 0.50 (0.17) \\ 
   &  & 30 & 0.90 (0.09) & 0.45 (0.08) & 0.64 (0.21) & 0.58 (0.13) & 0.92 (0.08) & 0.42 (0.07) \\ 
   &  & 100 & 0.90 (0.04) & 0.45 (0.04) & 0.74 (0.09) & 0.53 (0.06) & 0.92 (0.04) & 0.42 (0.03) \\ 
   \hline
\end{tabular}
\end{table}

\paragraph{\emph{p}-hacking} The simulation results for the \textit{p}-hacking model are in Table \ref{tab:Simulation_ph}. As before, the largest differences are in the most difficult case of $\tau = 0.5$, while the two models tend to agree in the more realistic case of $\tau = 0.1$. When $\tau = 0.5$ the publication bias model severely underestimates $\theta_0$, even getting the sign wrong in some instances. This should not come as a surprise given the interpretation of $\theta_0$ in the publication bias model, but shows that we should be cautious in interpreting the $\theta_0$ estimates. In basically all cases the $p$-hacking model outperforms the uncorrected model, with the latter surprisingly working better than the publication bias model when the effect size $\theta_0$ is large (0.8).

\begin{table}[ht]
\centering
\caption{{\bf \textit{p}-hacking.} Posterior means and 
                    standard deviations from the \textit{p}-hacking, 
                    publication bias, and uncorrected models when the data are simulated 
                    from the \textit{p}-hacking model with cut-offs at
                    $0.025$ and $0.05$, with \textit{p}-hacking probabilities
                    equal to $0.6$, $0.3$, and $0.1$ in the intervals
                    $[0, 0.025)$, $[0.025, 0.05)$, and $[0.5, 1]$} 
\label{tab:Simulation_ph}
\begin{tabular}{lllrrrrrr}
   \multicolumn{3}{r}{\textbf{True values}} & 
       \multicolumn{2}{c}{\textbf{\textit{p}-hacking model}} &
       \multicolumn{2}{c}{\textbf{Publication bias model}} &
       \multicolumn{2}{c}{\textbf{Uncorrected model}}\\$\tau$ & $\theta_0$ & $n$ & \multicolumn{1}{c}{$\widehat{\theta_0}$} & \multicolumn{1}{c}{$\widehat{\tau}$} & \multicolumn{1}{c}{$\widehat{\theta_0}$} & \multicolumn{1}{c}{$\widehat{\tau}$} & \multicolumn{1}{c}{$\widehat{\theta_0}$} & \multicolumn{1}{c}{$\widehat{\tau}$} \\ 
   \hline
\multirow{9}{*}{$0.1$} & \multirow{3}{*}{$0$} & 5 & -0.06 (0.14) & 0.29 (0.07) & 0.04 (0.06) & 0.17 (0.05) & 0.29 (0.06) & 0.16 (0.09) \\ 
   &  & 30 & -0.02 (0.08) & 0.13 (0.05) & 0.01 (0.07) & 0.07 (0.03) & 0.29 (0.02) & 0.05 (0.03) \\ 
   &  & 100 & 0.00 (0.05) & 0.10 (0.04) & 0.00 (0.05) & 0.05 (0.02) & 0.29 (0.01) & 0.03 (0.02) \\ 
   \cdashline{3-9}
 & \multirow{3}{*}{$0.2$} & 5 & 0.12 (0.16) & 0.29 (0.09) & 0.10 (0.06) & 0.21 (0.06) & 0.35 (0.05) & 0.13 (0.04) \\ 
   &  & 30 & 0.18 (0.06) & 0.12 (0.05) & 0.15 (0.06) & 0.09 (0.03) & 0.34 (0.02) & 0.04 (0.02) \\ 
   &  & 100 & 0.20 (0.04) & 0.09 (0.04) & 0.17 (0.05) & 0.08 (0.03) & 0.34 (0.01) & 0.02 (0.01) \\ 
   \cdashline{3-9}
 & \multirow{3}{*}{$0.8$} & 5 & 0.79 (0.08) & 0.18 (0.09) & 0.65 (0.14) & 0.30 (0.13) & 0.79 (0.08) & 0.17 (0.07) \\ 
   &  & 30 & 0.80 (0.03) & 0.10 (0.04) & 0.79 (0.03) & 0.10 (0.04) & 0.80 (0.03) & 0.10 (0.04) \\ 
   &  & 100 & 0.80 (0.02) & 0.10 (0.02) & 0.80 (0.02) & 0.10 (0.02) & 0.80 (0.02) & 0.10 (0.02) \\ 
   \cline{2-9}
\multirow{9}{*}{$0.5$} & \multirow{3}{*}{$0$} & 5 & 0.08 (0.22) & 0.47 (0.19) & 0.01 (0.12) & 0.37 (0.19) & 0.36 (0.12) & 0.29 (0.18) \\ 
   &  & 30 & 0.08 (0.09) & 0.43 (0.08) & -0.24 (0.19) & 0.35 (0.10) & 0.36 (0.05) & 0.24 (0.09) \\ 
   &  & 100 & 0.07 (0.06) & 0.44 (0.04) & -0.33 (0.14) & 0.37 (0.06) & 0.37 (0.03) & 0.24 (0.05) \\ 
   \cdashline{3-9}
 & \multirow{3}{*}{$0.2$} & 5 & 0.19 (0.24) & 0.50 (0.20) & 0.05 (0.13) & 0.42 (0.22) & 0.43 (0.13) & 0.30 (0.20) \\ 
   &  & 30 & 0.24 (0.09) & 0.47 (0.08) & -0.20 (0.19) & 0.46 (0.09) & 0.46 (0.05) & 0.28 (0.08) \\ 
   &  & 100 & 0.23 (0.05) & 0.47 (0.04) & -0.27 (0.16) & 0.47 (0.06) & 0.45 (0.03) & 0.29 (0.04) \\ 
   \cdashline{3-9}
 & \multirow{3}{*}{$0.8$} & 5 & 0.72 (0.19) & 0.60 (0.19) & 0.35 (0.20) & 0.73 (0.19) & 0.79 (0.15) & 0.51 (0.17) \\ 
   &  & 30 & 0.78 (0.09) & 0.52 (0.07) & 0.36 (0.23) & 0.67 (0.11) & 0.83 (0.07) & 0.44 (0.06) \\ 
   &  & 100 & 0.80 (0.05) & 0.50 (0.04) & 0.42 (0.20) & 0.65 (0.09) & 0.85 (0.04) & 0.43 (0.03) \\ 
   \hline
\end{tabular}
\end{table}

\section{Examples}\label{sect:examples}
In this section we apply the models on the two meta-analyses of \citet{cuddy2018p} and \citet{anderson2010violent}. As in the simulation study, we use normal models for each effect size with one-sided significance cut-off at $0.025$ and $0.05$ for both models. We use the same priors as we did in the simulation study. To compare the fit of the models we use the leave-one-out cross-validation information criterion (\textsc{LOOIC}) \citep{loo_article}, calculated using the R package $\mathtt{loo}$ \citep{loo}. LOOIC equals $-2\cdot\textsc{elpd}_{\textsc{loo}}$, where $\textsc{elpd}$ is the expected log pointwise predictive density for a new data set and $\textsc{elpd}_\textsc{loo}$ is an estimate of this quantity by leave-one-out cross validation. Just as the \textsc{AIC}, smaller values indicate better model fit. As for the simulation study, the analyses have been done with the $\mathtt{R}$ package $\mathtt{publipha}$ \citep{publipha}, which in turn uses \texttt{STAN} \citep{Carpenter2017-cf}. Each model has been estimated with $8$ chains. The code used to run the examples can be found in the Online Supporting Information and in an OSF repository (\url{https://osf.io/tx8qn/}).


\subsection{Power posing\label{subsec:cuddy2018}}

\citet{cuddy2018p} conducted a meta-analysis of a of power posing, an alleged phenomenon where adopting expansive postures has positive psychological feedback effects. Their meta-analysis is not conventional, but a \textit{p}-curve analysis \citep{simonsohn2014p}. A \textit{p}-curve analysis is not based on estimated effect sizes and standard errors, but directly on \textit{p}-values. The data from \citet{cuddy2018p} can be accessed via the Open Science Framework (\url{https://osf.io/pfh6r/}). Here we only consider studies with outcome \enquote{mean difference}, design \enquote{2 cell}, and test statistic that is either $F$ or $t$. The $F$-statistics are all with $1$ denominator degree of freedom, and the root of these are distributed as the absolute value of a $t$-distributed variable. The $t$-values and the roots of the $F$-statistics are converted to standardized mean differences by using $d = t\sqrt{2/\nu}$, where $\nu$ is the degrees of freedom for the $t$-test. The standardized mean differences are to the left in Figure \ref{fig:cuddy2017}. Note the outlier $x_{12} = 1.72$. As it has a large effect on all the models, we analyze the data both with and without $x_{12}$.

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.49\textwidth]{plots/cuddy2018}\includegraphics[width=0.49\textwidth]{plots/cuddy2018_posterior}
\par\end{centering}
\caption{\label{fig:cuddy2017} \textbf{(left)} Effect sizes for the power posing example. The dotted black line is $1.96/\textrm{sd}$ and the dashed black line is $1.64/\textrm{sd}$. The ticks on the right hand side are
the meta-analytic means: $0.48$ is from the uncorrected model, $0.17$ is the mean of the selected effect size distribution under the \textit{p}-hacking model, while $-0.06$ is the mean under the publication bias model. \textbf{(right)} Posterior densities for $\theta_{2}$ in the power posing example. The dashed density belongs to the \textit{p}-hacking model, the dotted density to the publication bias model, and the solid density to the uncorrected model. The point $x_{2}=0.62$ is marked for reference.}
\end{figure}

The estimates of the \textit{p}-hacking model, the publication bias model, and the uncorrected meta-analysis models are in Table \ref{tab:cuddy2018}. According to the LOOIC the corrected models account much better for the data than the uncorrected model. Both the \textit{p}-hacking model and the publication bias models estimate
larger $\tau$s and smaller $\theta_{0}$s than the classical model, with the publication bias model estimating the surprising $\theta_{0}\approx0$. But recall the results of the simulation study, where the publication bias model severely underestimates $\theta_0$ when the \textit{p}-hacking model is true.

The publication bias selection affects not only the observed $x_{i}$s, but also the $\theta_{i}$s. As a consequence, the posterior mean of the selected effect size distribution (this equals $0.37$, is not shown in the table, and equals the average of the posterior means for the $\theta_{i}$s) is much closer to the uncorrected model's estimate than the \textit{p}-hacked estimate. This effect can be most easily understood by looking at a specific $\theta$, for example the $\theta_2$ reported in the right plot of Figure \ref{fig:cuddy2017}, where $x_{2}=0.62$. In this case, the publication bias posterior for is close to the uncorrected posterior even though $\theta_0 \approx 0$. On the other hand, the \textit{p}-hacking model pushes $0.62$ down to $0.17$, towards the meta-analytic mean of $0.18$.

Finally, the surprisingly low value for $\theta_0$ obtained with the publication bias model can be a side effect of the presence of the outlier $x_{12} = 1.72$. Its presence on the right tail of an hypothetical true effect size distribution implies unobserved low and negative effects not reported due to publication bias. When the outlier is removed from the analysis, the estimate of $\theta_{0}$ goes up and agrees with the estimate from the \textit{p}-hacking model, which does not change. Once the outlier is removed, the fit of the publication bias model increases tremenduously, reaching a level close to that of the \textit{p}-hacking model. Moreover, the estimates of $\tau$ are strongly affected by the removal of $x_{12}$. In particular, the estimate of $\tau$ decreases from $0.45$ to $0.09$ in the \textit{p}-hacking model.

%% <<<<< Example table start (\label{tab:cuddy2018)
\input{tables/cuddy2018.tex}
%% example table end >>>>>

In conclusion, the \textit{p}-hacking and publication bias models suggest there is selection bias in these studies. Both models have much better fit than the uncorrected one and it is reasonable to accept their parameter estimates as more realistic. Nonetheless, both models agree on a value of $\theta_{0}$ that is likely to be different from $0$. The results of Table \ref{tab:cuddy2018} supports \citet{cuddy2018p}'s conclusion that there is evidence for some positive effect of power posing. The \textit{p}-hacking model does not suffer the presence of an outlier, and, in contrast to the publication bias model, provides similar results with and without $x_{12}$ in the data.

\subsection{Violent video games\label{subsec:Anderson}}

\citet{anderson2010violent} conducted a large meta-analysis on the effects of violent video games on seven negative outcomes such as aggressive behavior and aggressive cognition. As part of their analysis, they classified some experiments as best practice experiments \citep[for more details, see Table 2 of][]{anderson2010violent}. Suspecting publication bias, \citet{hilgard2017overstated} reanalysed the data using an array of tools to detect and adjust for publication bias. For the outcome variable aggressive cognition, \citet{hilgard2017overstated} noted that \enquote{Application of best-practices criteria seems to emphasize statistical significance, and a knot of experiments just reach statistical significance}. The data can be found on the web \citep{Hilgard2017} and are visualised to the left in Figure \ref{fig:anderson2010}. In the plot, the best practice experiments are represented by solid circles, all other experiments by hollow squares. An outlier $x=1.33$ has been removed from the data set, and excluded from our analyses. Its removal substantially improves the fit for all the models. 

In this example we fit the three models (\textit{p}-hacking, publication bias and uncorrected models) to three data subsets (all experiments, only best practice experiments, without best practice experiments). The outcome variable is aggressive behavior. Our the aim is to answer the following: (1) What are the parameter estimates, in each subset, for each model? (2) Which model has the best fit? (3) Do we have a reason to believe the best practice experiments are drawn from a different underlying distribution than the other experiments, as \citet{hilgard2017overstated} and the top left plot of Figure \ref{fig:anderson2010} suggest? (4) Is there a large difference between the posterior for $\theta_{0}$ and the mean posterior for the $\theta_{i}$s, as we saw in the previous example?

%% <<<<< Example table start (\label{tab:anderson2010)
\input{tables/anderson2010.tex}
%% example table end >>>>>

The first three questions can be answered by looking at Table \ref{tab:Anderson2010}. The estimates of $\theta_{0}$ are approximately the same for the publication bias and \textit{p}-hacking models, and roughly half of the uncorrected estimate in all cases. In particular, when all experiments or only the best experiments are considered, there is a noticeable difference. In these two cases, the LOOICs suggest that some \textit{p}-hacking or publication bias is present, as they are smaller than the LOOIC for the uncorrected models. Although the publication bias model seems to work slightly better than the \textit{p}-hacking model, we can state that the two models agree and we have little reason to prefer one to the other. Basically, we can interpret this as converging evidence that the parameter estimates obtained with these two models for $\theta_{0}$ and $\tau$ are in the ballpark of their true values.

Interestingly, when we exclude the experiments not considered best practice by \citet{anderson2010violent}, the differences between the estimates provided by the corrected and uncorrected models reduce and the LOOICs are almost the same. The question is if the differences between best practice and non-best practice studies reflect a different underlying distribution or not. To answer this question, let us take a look at the posterior densities for $\theta_{0}$ when all experiments are included, as reported in the top right plot of Figure \ref{fig:anderson2010}. In this case, the posterior distributions computed with the \textit{p}-hacking and publication bias models are similar (dashed and dotted lines, respectively), which strengthens the agreement seen in Table \ref{tab:Anderson2010}. There is no large difference between the posterior for $\theta_{0}$ and the mean posterior for the $\theta_{i}$s as in the previous example. The answer to question (4) is therefore no.

Back to question (3), we have good reasons to believe the best practice experiments have been drawn from a different underlying distribution than the other experiments if there is negligible overlap between the posteriors for the parameters $\theta_{0}$. The uncorrected model supports this hypothesis (bottom right plot of Figure \ref{fig:anderson2010}), but the \textit{p}-hacking and publication bias models to do not. See the bottom left plot of Figure \ref{fig:anderson2010} for the posteriors for $\theta_0$ in the publication bias model (those obtained with the \textit{p}-hacking model are indistinguishable). In this case, the overlap between the posteriors for the different subsets is not negligible, and there is no evidence against hypotheses of equal $\theta_0$s in both groups. The same conclusion can be reached from Table \ref{tab:Anderson2010} by looking at the posterior standard deviations and posterior means.

\begin{figure}
\includegraphics[width=0.49\textwidth]{plots/anderson2010}
\includegraphics[width=0.49\textwidth]{plots/anderson_posterior}
\includegraphics[width=0.49\textwidth]{plots/anderson_posterior_1}
\includegraphics[width=0.49\textwidth]{plots/anderson_posterior_2}
\caption{\label{fig:anderson2010}Violent video games example with outcome variable aggressive behavior. \textbf{(top-left)} Effect sizes. The dotted black line is $1.96/\textrm{sd}$ and the dashed black line is $1.64/\textrm{sd}$. The ticks on the right hand side are the uncorrected meta-analytical means for each group: $0.29$ for the best practices group, $0.08$ for the rest. The outlier $x=1.33$ has been removed from the plot.
\textbf{(top-right)} Posterior densities for $\theta_{0}$ with all experiments included. The dashed density belongs to the \textit{p}-hacking model, the dotted to the publication bias model, and the solid to the uncorrected model. \textbf{(bottom-left)} Posterior densities for $\theta_{0}$ from the publication bias model. The solid curve is the model with all experiments, the dotted curve the model with the best practice experiments, and the dashed line the model without the best experiments. The posteriors for the \textit{p}-hacking model are similar to this one. \textbf{(bottom-right)} Posterior densities for $\theta_{0}$ (solid line: all experiments; dotted line: best practice experiments only; and dashed line without the best experiments) from the uncorrected meta-analysis model.}
\end{figure}

\section{Concluding remarks}\label{sect:conclusions}

In this paper we studied two models to handle the effect of \textit{p}-hacking and publication bias. Although the \textit{p}-hacking model worked really well in the simulation study, we have to admit that the \textit{p}-hacking scenario described in Section \ref{subsect:p-hacking} is less plausible than the publication bias scenario of Section \ref{subsect:publicationBias}. First, the assumption of Bob's \textit{p}-hacking omnipotence is strong. For while some researchers are able \textit{p}-hackers, most give up at some point. Does truncation actually model \textit{p}-hacking in the wild? Analysing \textit{p}-hacking is hard without serious simplifying assumptions. The model we proposed is interpretable and implementable, and it appears to work well in practice, as one can see in the examples of Section \ref{sect:examples}. That said, there is space for further development of models for \textit{p}-hacking.

Regarding possible further development, we are often interested in understanding and modelling the sources of heterogeneity in a meta-analysis \citep{thompson1994systematic}. A way to do this is to let $\theta_{i}$ linearly depend on covariates, in the meta-analysis context known as moderators. If we extend the one-sided discrete models publication bias and \textit{p}-hacking models to include covariates, we will be able to estimate their effect while keeping the \textit{p}-hacking probability or the selection probability fixed. Another option is to allow the \textit{p}-hacking probability or the selection probability to depend on covariates themselves. For instance, the difficulty of \textit{p}-hacking is likely to increase with $n$, the sample size of the study. Similarly, the selection probability is also likely to be influenced by $n$; for example when $n$ is large, null-effects are more publishable.


% Although the common practice in meta-analysis studies is to treat the standard deviations as nuisance parameter , the actual tests usually contain an estimate of the standard error. If the $\eta_i$s are in the selection set, they can influence the study's acceptance. 

% Further modification to the models can be obtained by modifying their Xelection sets. For example, in the publication bias model, the nuisance parameter $\eta_i$ (which can include, e.g., the standard deviation $\sigma_i$) could be incorporated. A possible modification of the \textit{p}-hacking model consists in putting $\theta_i$ inside the Xelection set, which makes the researcher draw new $\theta_i$s every time he attempts a \textit{p}-hack. This could be used to model scenarios where the hypothesis is not known in advance by the researchers.

We saw in the simulations and in Example \ref{subsec:cuddy2018} that the publication bias and the \textit{p}-hacking models can give remarkably different results even with similar priors and the same $\alpha$ vector. A way to react to this situation is to choose the best-fitting model in terms of, for example, LOOIC. Nevertheless, this may result dangerous, and one should be caution, to not risk to over-interpret the results. More safely, one can present the results of both models and try to understand the differences between them, as we did in the examples of Section \ref{sect:examples}. In the publication bias model, it is especially important to be aware of the interpretation of $\theta_{0}$ as the mean of the underlying effect size distribution, not the effect size distribution of the observed studies. Therefore, the best response to the question \enquote{Should one use the \textit{p}-hacking and publication bias model?} is probably \enquote{Use both!}

Finally, it would be interesting to model publication bias and \textit{p}-hacking at the same time:
\begin{quote}
Bob \textit{p}-hacks his research to a \textit{p}-value drawn from $\omega$ and sends it to Alice's journal. Alice accepts the paper with probability $w(u_i)$. Every rejected study is lost.
\end{quote}
In this scenario the original density $f^{\star}(x_{i}\mid\theta_{i},\eta_{i})$ is transformed twice: First by \textit{p}-hacking, then by publication bias. The resulting model is
$$
f(x_{i}\mid\theta_{i},\eta_{i})\propto w(u_i)\int_{[0,1]}f_{[0,\alpha]}^{\star}(x_{i}\mid\theta_{i},\eta_{i})d\omega(\alpha).
$$
This is a reasonable model, but its normalizing constant is hard to calculate, even when $\omega$ is discrete and $w$ is a step function. Additional work on this problem is required.

\bibliographystyle{biom}
\bibliography{edited.bib}

\label{lastpage}
\end{document}
